#### Описание
В данном проекте демонстрируется пайплайн автоматической выгрузки данных о землетрясениях с публичного API с последующей передачей сначала в Data Lake (Minio S3), откуда данные будут передаваться в Data Warehouse (PostgreSQL) и, в конечном итоге, попадать на витрины данных в BI-систему (Metabase). Процесс автоматизирован с помощью Apache Airflow, все сервисы будут запускаться из Docker контейнеров.

Для запуска контейнеров потребуется Docker (с применением `docker-compose`), а также DBeaver или PGAdmin для создания схем для таблиц; всё остальное будет открываться в браузере.

**[Cсылка на API](https://earthquake.usgs.gov)**

Идея и схема проекта взяты из данного гайда: **[YouTube](https://www.youtube.com/watch?v=MQPHgUQvKnI)**

#### Схема проекта

```mermaid
    flowchart LR
    subgraph API
        direction LR
        API_E["Earthquake API"]
    end

    subgraph ETL
        direction LR
        AirFlow
    end

    subgraph Storage
        direction LR
        S3
    end

    subgraph DWH
        direction LR
        subgraph PostgreSQL
            direction LR
            subgraph model
                direction LR
                ods["ODS Layer"]
                dm["Data Mart Layer"]
            end
        end
    end

    subgraph BI
        direction LR
        MetaBase
    end

    API_E -->|Extract Data| AirFlow
    AirFlow -->|Load Data| S3
    S3 -->|Extract Data| AirFlow
    AirFlow -->|Load Data to ODS| ods
    ods -->|Extract Data| AirFlow
    AirFlow -->|Transform and Load Data to DM| dm
    dm -->|Visualize Data| MetaBase
    style API fill: #FFD1DC, stroke: #000000, stroke-width: 2px
    style ETL fill: #D9E5E4, stroke: #000000, stroke-width: 2px
    style Storage fill: #FFF2CC, stroke: #000000, stroke-width: 2px
    style DWH fill: #C9DAF7, stroke: #000000, stroke-width: 2px
    style PostgreSQL fill: #E2F0CB, stroke: #000000, stroke-width: 2px
    style BI fill: #B69CFA, stroke: #000000, stroke-width: 2px
```

#### Подготовка и запуск (Windows)
1) Скопировать папку `EQ_docker_setup` из данного репозитория
2) Через терминал перейти в папку `EQ_docker_setup` на вашем ПК, далее провести сборку и инициализацию образа командой: 
   ` docker compose up airflow-init `
3) Если со временем вывод терминала окончится строкой `airflow-init-1 exited with code 0` без ошибок, значит сборка образа Airflow прошла успешно и можно в дальнейшем запускать Docker контейнер следующей командой:
   ` docker-compose up -d `
4) Теперь можно войти в Airflow через браузер по адресу: http://localhost:8080 (логин/пароль по умолчанию: `airflow`)
5) В хранилище Minio можно войти по адресу: http://localhost:9000 (логин/пароль по умолчанию: `minioadmin`)
6) В BI-систему Metabase можно перейти по адресу: http://localhost:3000 (нужно будет заполнить форму регистрации, на этапе добавления подключения к БД выберите "добавлю позже")

Теперь, когда все сервисы запущены, можно приступить к их настройке и запуску пайплайна.

#### Создание хранилища в Minio
1) На главной странице Minio или в меню Buckets нажимаем `Create Bucket` и присваиваем ему имя `storage` (обязательно в нижнем регистре)

    ![Кнопка создания хранилища](./img/01_minio_create_bucket_button.png)

    ![Создание хранилища](./img/02_minio_bucket_created.png)
    
2) В меню Access Keys создаём новый ключ доступа к хранилищу (ничего дополнительно выбирать не надо, для наших целей ключ будет общий и бессрочный; полученные `Access Key` и `Secret Key` сохраните отдельно)

    ![Кнопка создания ключа](./img/03_minio_create_access_key_button.png)

    ![Создание ключа доступа](./img/04_minio_access_key_creation.png)

#### Создание схем в Postgres
1) Через DBeaver или PGAdmin создаём новое подключение к контейнеру `postgres_dwh` в Docker (порт - 5462 (так как мы подключаемся вне Docker контейнера; порт 5432 (по умолчанию) может быть занят, если у вас локально установлен PostgreSQL), пароль - `postgres`)

    ![Подключение к Postgres](./img/08_postgres_dwh_connection.png)

2) Открываем новый SQL редактор для данного подключения и добавляем необходимые схемы
    ```
    CREATE SCHEMA ods;
    CREATE SCHEMA dm;
    CREATE SCHEMA stg;
    ```

    Пояснение: 
      - схема `ods` - operational data storage или слой оперативных данных, которые мы получаем из хранилища Minio S3
      - схема `stg` - слой временных таблиц для передачи в витрины данных
      - схема `dm` - data marts или слой витрин данных, которые будут визуализироваться в Metabase

3) Затем добавляем таблицу для хранения данных о землетрясениях (везде указан тип `varchar` для упрощения, но в идеале нужно поработать над приведением типов на этапе передачи сырых данных из хранилища Minio в Postgres)
    ```
    CREATE TABLE ods.fct_earthquake
    (
        time varchar,
        latitude varchar,
        longitude varchar,
        depth varchar,
        mag varchar,
        mag_type varchar,
        nst varchar,
        gap varchar,
        dmin varchar,
        rms varchar,
        net varchar,
        id varchar,
        updated varchar,
        place varchar,
        type varchar,
        horizontal_error varchar,
        depth_error varchar,
        mag_error varchar,
        mag_nst varchar,
        status varchar,
        location_source varchar,
        mag_source varchar
    )
    ```

4) Также добавляем таблицу подсчёта суммарного кол-ва землетрясений в день (это будет витриной данных в Metabase)
    ```
    CREATE TABLE dm.fct_count_day_earthquake AS 
    SELECT time::date AS date, count(*)
    FROM ods.fct_earthquake
    GROUP BY 1
    ```

5) И, наконец, добавляем таблицу подсчёта средней магнитуды землетрясений в день
    ```
    CREATE TABLE dm.fct_avg_day_earthquake AS
    SELECT time::date AS date, avg(mag::float)
    FROM ods.fct_earthquake
    GROUP BY 1 
    ```

#### Настройка Airflow
1) На главной странице Airflow переходим в меню Admin -> Variables и нажимаем `Add Variable`, чтобы добавить две переменные - для ключа доступа и для секретного ключа хранилища в Minio

    ![Кнопка создания переменной](./img/05_airflow_add_variable_button.png)

    ![Создание переменной](./img/06_airflow_add_variable_window.png)

    ![Готовые переменные](./img/07_airflow_created_variables.png)

2) Аналогичным образом создаём переменную `pg_password` со значением `postrgres` (как указано в docker-compose файле для сервиса `postgres_dwh`)

3) В меню Admin -> Connections добавляем подлкючение к `postgres_dwh` (порт - 5432 (так как мы подключаемся внутри Docker контейнера), пароль - `postrgres`)

    ![Создание подключения к Postgres в Airflow](./img/09_airflow_postgres_dwh_connection.png)

4) Переходим в меню Dags и проверяем, что все DAG добавились в Airflow без ошибок

    ![Airflow DAGs](./img/10_airflow_all_dags.png)

5) Запускаем наши DAG в следующем порядке (подождите, пока DAG пройдёт все шаги без ошибок, предже чем запускать следующий): `raw_from_api_to_s3`, `raw_from_s3_to_pg,` `fct_count_day_earthquake`, `fct_avg_day_earthquake_mag`

Может возникнуть такая проблема: из-за разницы часовых поясов полученный запрос может быть пустым (так как данные за текущий день ещё не появились в API), плюс ждать несколько дней подряд для проверки работы DAG было бы расточительно. Поэтому мы запустим наши DAG задним числом.

Для этого на странице Dags выбираем нужный нам DAG (например `raw_from_api_to_s3`) и нажимаем кнопку `Trigger`

![DAG Trigger](./img/12_airflow_dag_trigger_button.png)

После чего заполняем даты начала и конца выполнения, нажимаем кнопку `Run Backfill`

![Run Backfill](./img/13_airflow_backfill_dag.png)

Проделываем то же самое для остальных DAG.

#### Настройка Metabase и добавление витрин
1) На главной странице Metabase переходим в меню "Базы данных", нажимаем кнопку "Добавить базу данных" и заполняем поля в окне подключения (пароль - `postgres`, порт - 5432, так как мы подключаемся внутри контейнера по имени хоста в Docker, а не извне, как было в случае DBeaver)

    ![Подключение Postgres DWH к Metabase](./img/11_metabase_pg_dwh_connection.png)

    **Примечание:** Если не хотите, чтобы в Metabase попал слой оперативных данных, а только витрины, при создании подключения выберите только схему `dm`

2) После успешного подключения, выходим из режима администрирования Metabase и переходим к добавленной базе данных `dwh`

    ![БД DWH в Metabase](./img/14_metabase_dwh_db.png)

3) Переходим в схему `dm` и теперь мы можем посмотреть наши витрины данных по кол-ву землетрясений и средней магнитуде

    ![Витрины данных в Metabase](./img/15_metabase_dwh_data_marts.png)

    Содержимое витрин данных:

    ![Витрина кол-ва землетрясений по дням в Metabase](./img/16_metabase_fct_cnt_day_eq.png)

    ![Витрина средней магнитуды по дням в Metabase](./img/17_metabase_fct_avg_day_mag_eq.png)

4) Также мы можем посмотреть быструю сводку по нашим данным в схеме `ods`, предлагаемую Metabase из главного меню

    ![Кнопка краткой сводки от Metabase](./img/18_metabase_quick_look_button.png)

    Результат:

    ![Краткая сводка по-умолчанию от Metabase](./img/19_metabase_quick_look_for_ods_schema.png)

    На основе данных в схеме `ods` можно создавать свои запросы и дашборды. Или же использовать только витрины данных из схемы `dm` (см. примечание к пункту 1).
    
#### Итоги

Теперь у нас имеется полноценный автоматизированный ETL пайплайн, избавляющий от рутинных задач по выгрузке данных из API и последующей передаче в витрины данных в BI систему.

Данный проект можно доработать по некоторым пунктам, например:

- Можно избавиться от Minio и передавать результаты запроса от API напрямую в Postgres, если данные из API достаточно хорошо стандартизированы и подчищаются заранее (получится обычный Data Warehouse)
- Можно доработать приведение типов при передаче сырых данных из Minio в Postgres, т. к. специализированные типы упрощают работу с запросами и занимают меньше памяти
- Можно добавить новые DAG для создания других витрин данных (например, места с наибольшим кол-вом землетрясений или с самой большой магнитудой)
- Можно переписать данный проект под работу с другим API и данными
